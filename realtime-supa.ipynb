{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139829b0-f80c-4495-aadc-b532333d099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130fadb-35a0-40a8-9385-869e6848160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dff9a-2a5b-4681-b5e6-f0551e2f7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "\n",
    "# Ensure you have a GPU available for this, as the model is quite large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed97d4d-8ce3-4414-8537-94526e882314",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd2bbb-2647-4012-8c0c-ded104e1f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our text iterator streamer and the Thread\n",
    "from supabase import create_client, Client\n",
    "SUPABASE_URL=\"https://klmxerppxsgwnrqsfivb.supabase.co\"\n",
    "SUPABASE_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImtsbXhlcnBweHNnd25ycXNmaXZiIiwicm9sZSI6ImFub24iLCJpYXQiOjE2ODg0NjcyMTQsImV4cCI6MjAwNDA0MzIxNH0.sRGApC0CBEyO1mgpnP-idLnCE68vxneQR9iO3jDnX5M\"\n",
    "\n",
    "url: str = SUPABASE_URL\n",
    "key: str = SUPABASE_KEY\n",
    "\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "# create our stream object, this takes the same\n",
    "# arguments as the TextStreamer object from before\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "# next define our prompt and convert them into input ids\n",
    "prompt = '''I'm going to tell you a long winded story about a cat now. So once upon a time '''\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# now we need to define a new thread for our code. So we'll now have 2 threads,\n",
    "# running parallel, this new thread will just be responsable for having the\n",
    "# model generate the tokens, and our main thread will iterate on the streamer\n",
    "# object and process the text as it's generated.\n",
    "\n",
    "# We won't go over the details on how to use the Thread object\n",
    "# but just know that we're setting it up to call the model.generate with\n",
    "# the same arguments as our last example. \n",
    "thread = Thread(target=model.generate, \n",
    "                kwargs={\"input_ids\": inputs['input_ids'],\n",
    "                        \"streamer\": streamer, \"max_new_tokens\": 100})\n",
    "thread.start() # now start the thread\n",
    "\n",
    "# so while our model is generate new tokens in the other thread we\n",
    "# can access them here, each new_text our streamer returns is the next\n",
    "# word/phrase generated from our model\n",
    "\n",
    "# for this example we'll both print out the new text and save it to a file\n",
    "f = open(\"./output.txt\", \"w\")\n",
    "out=\"\"\n",
    "for new_text in streamer:\n",
    "  out= str(out) + str(new_text)\n",
    "  print(new_text, end=\"\")\n",
    "  f.write(new_text)\n",
    "  data, count = supabase.table('stories').upsert({'id': 4616, 'story_parts': out}).execute()\n",
    "  #print(\"data:\",data,\"\\n count:\",count)\n",
    "f.close() # close our file\n",
    "thread.join() # join our thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7626-a896-4d3e-a3fa-dd74eed59ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7418808-996f-4edf-9186-d83d23dd6804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ba797-0058-4862-8889-0163fcedda48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
